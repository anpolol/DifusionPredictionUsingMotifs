{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import mean_squared_error \n",
    "import igraph\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "import regex as re\n",
    "\n",
    "from modules.support_functions import Utils\n",
    "from modules.Modularity import RecursiveModularity\n",
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "#supernoder files:\n",
    "\n",
    "#вариант, который считает РАЗНЫЕ типы мотивов одного и того же размера\n",
    "from SuperNoder_diff_types.manager import Manager as Manager_types \n",
    "\n",
    "#вариант, который считает все мотивы одного размера вместе\n",
    "from SuperNoder.manager import Manager as Manager\n",
    "\n",
    "from modules.support_functions import Utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "Read = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-intake",
   "metadata": {},
   "source": [
    "**Запустите или клетку ниже, чтоб загрузить и отфлитровать данные (дольше), или клетку ниже через одну, чтоб загрузить уже отфильтрованные данные (быстрее)**\n",
    "\n",
    "`type(graphs) = list of tuples: [(str(name),networkx(graph))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Read:\n",
    "    #reading all data\n",
    "    for root, dirs, files in os.walk(r\"C:\\Users\\anpolol\\Desktop\\DifusionPredictionUsingMotifs\\Data\\reply_networks\"):\n",
    "        datasets_names = files\n",
    "\n",
    "\n",
    "    to_train = ['badtattoos.json', 'gonewildcurvy.json','southpark.json','geology.json','hardwareswap.json','counterstrike.json','stopsmoking.json','memes.json','Feminism.json','introvert.json','Pizza.json','vegetarian.json','depression.json','CrazyIdeas.json','lifehacks.json','freedonuts.json','Bonsai.json','Colorado.json','GreenBayPackers.json','beertrade.json']\n",
    "\n",
    "    #in every graph there are 11 networks, all_nets consists of all nets  \n",
    "    nets_to_train=dict()\n",
    "    for dataset in to_train:\n",
    "        if dataset not in nets_to_train:\n",
    "            nets_to_train[dataset] = []\n",
    "            month_nets = json.load(open(r\"C:\\Users\\anpolol\\Desktop\\DifusionPredictionUsingMotifs\\Data\\reply_networks\\\\\"+str(dataset)))\n",
    "            nets_to_train[dataset]=nets_to_train[dataset] + month_nets\n",
    "\n",
    "    #фильтрация по размеру исходных графов\n",
    "    def func(inp):\n",
    "            from collections import Counter\n",
    "            import networkx as nx\n",
    "            dataset,values = inp\n",
    "            name = dataset.split('.')\n",
    "            graphs=[]\n",
    "            for i,net in enumerate(values):\n",
    "\n",
    "                t=list(net.values()) #extracting nodes from json files\n",
    "                nodes = (Counter([item for sublist in t for item in sublist] + list(net.keys())))\n",
    "\n",
    "                map_nodes=dict(zip(nodes, list(range(len(nodes))))) #нумеруем ники\n",
    "\n",
    "                g = nx.Graph()\n",
    "                for node in range(len(nodes)):\n",
    "                    g.add_node(node,label='Motif')\n",
    "\n",
    "                for node in net:\n",
    "                    for neigh in net[node]:\n",
    "                        g.add_edge((map_nodes[node]),(map_nodes[neigh])) #first node replied to the second   \n",
    "\n",
    "                for j,k in enumerate(nx.connected_components(g)):\n",
    "                    if j==0:\n",
    "                        g_new = g.subgraph(k)\n",
    "                        old_ind = sorted(g_new)\n",
    "                        new_ind = list(range(len(old_ind)))\n",
    "                        mapping = dict(zip(old_ind,new_ind))\n",
    "                        g_new = nx.relabel_nodes(g_new, mapping)\n",
    "                        if g_new.number_of_nodes()>50:\n",
    "                            graphs.append((name[0]+'.'+str(i)+'.'+str(j),g_new)) #i - месяц, j - номер кмпоненты связности\n",
    "            return graphs\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        res = executor.map(func, list(nets_to_train.items()))\n",
    "\n",
    "    graphs=[]\n",
    "    for i in res:\n",
    "        graphs+=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Read:\n",
    "    #Загрузка готовых данных\n",
    "    with open('all_graphs.pickle','rb') as f:\n",
    "        graphs=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-premium",
   "metadata": {},
   "source": [
    "# Характеристика отфильтрованных графов\n",
    "\n",
    "- число узлов \n",
    "- плотность\n",
    "- к-т кластеризации (общий, для графа)\n",
    "- ассортативность (по степени)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#конвертируем список кортежей в словарь для более быстрого счета\n",
    "new_names = list(map(lambda x: x.split('.')[0], list(zip(*graphs))[0]))\n",
    "new_graphs = list(zip(new_names, list(zip(*graphs))[1]))\n",
    "\n",
    "graphs_dict=dict()\n",
    "for name,g in new_graphs:\n",
    "    if name not in graphs_dict:\n",
    "        graphs_dict[name]=[]\n",
    "    graphs_dict[name].append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Характеристика графов по темам\n",
    "table_names=[]\n",
    "table_NN_mean=[]\n",
    "table_CC_mean=[]\n",
    "table_D_mean=[]\n",
    "table_DAC_mean=[]\n",
    "\n",
    "table_NN_std=[]\n",
    "table_CC_std=[]\n",
    "table_D_std=[]\n",
    "table_DAC_std=[]\n",
    "\n",
    "\n",
    "for k in graphs_dict:\n",
    "    number_of_nodes = []\n",
    "    clustering_coef=[]\n",
    "    density=[]\n",
    "    degree_assort=[]\n",
    "    graphs=[]\n",
    "\n",
    "    table_names.append(k)\n",
    "    \n",
    "    for gr in graphs_dict[k]:\n",
    "            #число узлов \n",
    "            number_of_nodes.append(gr.number_of_nodes())\n",
    "            #коэффициенты кластеризации ищутся следующим образом:\n",
    "            clustering_coef.append(np.mean(list(nx.clustering(gr).values())))\n",
    "            #плотность\n",
    "            density.append(nx.density(gr))\n",
    "            #ассортативность\n",
    "            degree_assort.append(nx.degree_assortativity_coefficient(gr))\n",
    "    table_NN_mean.append(np.mean(np.array(number_of_nodes)))\n",
    "    table_CC_mean.append(np.mean(np.array(clustering_coef)))\n",
    "    table_D_mean.append(np.mean(np.array(density)))\n",
    "    table_DAC_mean.append(np.mean(np.array(degree_assort)))\n",
    "    table_NN_std.append(np.std(np.array(number_of_nodes)))\n",
    "    table_CC_std.append(np.std(np.array(clustering_coef)))\n",
    "    table_D_std.append(np.std(np.array(density)))\n",
    "    table_DAC_std.append(np.std(np.array(degree_assort)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.DataFrame({'Name of dataset' : table_names,\n",
    "                                'Number of nodes (NN)' :table_NN_mean,\n",
    "                                'Clustering coefficient (CC)': table_CC_mean,'Density(D)':table_D_mean, 'Degree assortativity coefficient(DAC)':table_DAC_mean})\n",
    "std_df = pd.DataFrame({'Name of dataset' : table_names,\n",
    "                                'Number of nodes (NN)' :table_NN_std,\n",
    "                                'Clustering coefficient (CC)': table_CC_std,'Density(D)':table_D_std, 'Degree assortativity coefficient(DAC)':table_DAC_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-portugal",
   "metadata": {},
   "source": [
    "# Sampling and motif counter (SuperNoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from littleballoffur import DegreeBasedSampler,\\\n",
    "                               PageRankBasedSampler,\\\n",
    "                               RandomEdgeSampler,\\\n",
    "                               SnowBallSampler,\\\n",
    "                               ForestFireSampler,\\\n",
    "                               CommunityStructureExpansionSampler,\\\n",
    "                               ShortestPathSampler,\\\n",
    "                               RandomWalkSampler,\\\n",
    "                               RandomWalkWithJumpSampler,\\\n",
    "                               MetropolisHastingsRandomWalkSampler,\\\n",
    "                               NonBackTrackingRandomWalkSampler,\\\n",
    "                               CirculatedNeighborsRandomWalkSampler,\\\n",
    "                               CommonNeighborAwareRandomWalkSampler,\\\n",
    "                               LoopErasedRandomWalkSampler\n",
    "methods = [\n",
    "           #random node sampling\n",
    "           DegreeBasedSampler,\n",
    "           PageRankBasedSampler,\n",
    "\n",
    "           #Random Edge Sampling\n",
    "           RandomEdgeSampler,\n",
    "           SnowBallSampler,\n",
    "           CommunityStructureExpansionSampler,\n",
    "           ShortestPathSampler,\n",
    "            #Random-Walks Dased\n",
    "           RandomWalkSampler,\n",
    "           RandomWalkWithJumpSampler,\n",
    "           MetropolisHastingsRandomWalkSampler,\n",
    "           NonBackTrackingRandomWalkSampler,\n",
    "           CirculatedNeighborsRandomWalkSampler,\n",
    "           CommonNeighborAwareRandomWalkSampler,\n",
    "           LoopErasedRandomWalkSampler,\n",
    "            RecursiveModularity\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-conflict",
   "metadata": {},
   "source": [
    "# Запустите клетки ниже до заголовка \"MSE\" с флагом diff_types = True, а затем заново с diff_types=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_types=True #будут разделения на типы мотивов или только по размеру?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_max =  8 # максимальный размер мотивов\n",
    "\n",
    "find_motif = Utils.find_motifs_diff_types if diff_types else Utils.find_motifs_all_types\n",
    "        \n",
    "#подсчитаем распределение мотивов для исходных графов - надо для подсчета MSE\n",
    "\n",
    "d = datetime.now()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        res = executor.map(lambda x: find_motif(x,ms_max),graphs)\n",
    "print(datetime.now()-d)\n",
    "\n",
    "if diff_types:\n",
    "    motifs_full_graphs_f1_diff = dict()\n",
    "    motifs_full_graphs_f3_diff = dict()\n",
    "    for name, motifs,motifs_disjoint in res:\n",
    "        motifs_full_graphs_f1_diff[name] = motifs\n",
    "        motifs_full_graphs_f3_diff[name] = motifs_disjoint\n",
    "        with open('motifs_of_full_graphs_f1_diff.pickle','wb') as f:\n",
    "            pickle.dump(motifs_full_graphs_f1_diff,f)\n",
    "        with open('motifs_of_full_graphs_f3_diff.pickle','wb') as f:\n",
    "            pickle.dump(motifs_full_graphs_f3_diff,f)\n",
    "else:\n",
    "    motifs_full_graphs_f1 = dict()\n",
    "    motifs_full_graphs_f3 = dict()\n",
    "    for name, motifs,motifs_disjoint in res:\n",
    "        motifs_full_graphs_f1[name] = motifs\n",
    "        motifs_full_graphs_f3[name] = motifs_disjoint\n",
    "        with open('motifs_of_full_graphs_f1.pickle','wb') as f:\n",
    "            pickle.dump(motifs_full_graphs_f1,f)\n",
    "        with open('motifs_of_full_graphs_f3.pickle','wb') as f:\n",
    "            pickle.dump(motifs_full_graphs_f3,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def find_motifs(inp,ms_max=8,diff_types = True):  # возвращает motifs f1 И f3. Разделение на разные типы мотивов. Размеры мотивов 3 и 4\n",
    "        find_motif = Utils.find_motifs_diff_types if diff_types else Utils.find_motifs_all_types\n",
    "        method, number_of_nodes = inp\n",
    "        \n",
    "        motifs_f1 = dict()\n",
    "        motifs_f3 = dict()\n",
    "        for graph in graphs:\n",
    "            if number_of_nodes <= graph[1].number_of_nodes():\n",
    "                if method == RecursiveModularity:\n",
    "\n",
    "                    sim_tuple = [tuple([k[0], k[1], 1]) for k in graph[1].edges()]\n",
    "                    graph_i = igraph.Graph.TupleList(sim_tuple, weights=True)\n",
    "\n",
    "                    rm = RecursiveModularity(graph_i, min_modularity=0.1, min_nodes=number_of_nodes,\n",
    "                                             modularity_iters=10)\n",
    "                    G_tree = rm.calculate_tree()\n",
    "                    # finding the index of module with the closest number of nodes to the needed one\n",
    "                    sg_names = list(rm.node_popualation.keys())  # all indices of all modules\n",
    "\n",
    "                    me = len(max(list(map(lambda x: x.split('_'), sg_names)), key=len))\n",
    "\n",
    "                    if len(min(list(map(lambda x: x.split('_'), sg_names)), key=len)) != me:\n",
    "\n",
    "                        indices = [i for i, j in enumerate(sg_names) if len(j.split('_')) == me]\n",
    "                        sg_names_max = [sg_names[i] for i in indices]\n",
    "\n",
    "                        def func(x, prev):\n",
    "                            if x:\n",
    "                                return prev + x\n",
    "                            else:\n",
    "                                return prev\n",
    "\n",
    "                        d = []\n",
    "                        candidates = reduce(func, list(\n",
    "                            map(lambda x: re.findall('_'.join(x[1].split('_')[:-6]) + '_' + '\\d{1,3}' + '_', x[0]),\n",
    "                                product(sg_names, sg_names_max))), d)\n",
    "                        l_cand_sep = list(Counter(candidates).keys())\n",
    "\n",
    "                    else:\n",
    "                        l_cand_sep = sg_names\n",
    "\n",
    "                    min_ = graph_i.vcount()\n",
    "                    min_ind = l_cand_sep[0]\n",
    "                    for i in (l_cand_sep):\n",
    "                        if len(rm.node_popualation[i]) > number_of_nodes and len(rm.node_popualation[i]) < min_:\n",
    "                            min_ = len(rm.node_popualation[i])\n",
    "                            min_ind = i\n",
    "\n",
    "                    # finding the sample\n",
    "                    sample = graph_i.subgraph([x.index for x in graph_i.vs if x['name'] in rm.node_popualation[min_ind]])  # should write another index\n",
    "                    sample = sample.to_networkx()\n",
    "\n",
    "                    for node in sample.nodes:\n",
    "                        sample.add_node(node, label='Motif')\n",
    "                    _, motifs_sample, motifs_disjoint_sample = find_motif((graph[0], sample), ms_max)\n",
    "\n",
    "                    motifs_f1[graph[0] + '_0'] = motifs_sample\n",
    "                    motifs_f3[graph[0] + '_0'] = motifs_disjoint_sample\n",
    "\n",
    "                else:  # for other methods we should\n",
    "                    for s in range(10, 20):  # add repeat due to stochastic methods\n",
    "                        sampler = method(number_of_nodes, seed=s)\n",
    "                        sample = sampler.sample(graph[1])\n",
    "                        if not sample.nodes[list(sample.nodes)[0]]:  # если первая вершина в списке нулевая.\n",
    "                            for node in sample.nodes:\n",
    "                                sample.add_node(node, label='Motif')\n",
    "\n",
    "                        _, motifs_sample, motifs_disjoint_sample = find_motif((graph[0], sample),\n",
    "                                                                                                ms_max)\n",
    "\n",
    "                        motifs_f1[graph[0] + '_' + str(s - 10)] = motifs_sample\n",
    "                        motifs_f3[graph[0] + '_' + str(s - 10)] = motifs_disjoint_sample\n",
    "            else:\n",
    "                for s in range(10):\n",
    "                    motifs_f1[graph[0] + '_' + str(s)] = {}\n",
    "                    motifs_f3[graph[0] + '_' + str(s)] = {}\n",
    "        return number_of_nodes, motifs_f1, motifs_f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#получим словарь распределения мотивов для сэмплированных подграфов\n",
    "motifs_methods_f1 = dict()\n",
    "motifs_methods_f3 = dict()\n",
    "for method in methods:\n",
    "    name_of_method=str(method).split('.')[-1].split(\"'\")[0]\n",
    "    d = datetime.now()\n",
    "    motifs_methods_f1.setdefault(name_of_method,dict())\n",
    "    motifs_methods_f3.setdefault(name_of_method,dict())\n",
    "    \n",
    "    # here is a parallelization\n",
    "    r = 300\n",
    "    l = 10\n",
    "    step = 10\n",
    "    inp = list(zip([method]*int((r-l)/step),list(range(l,r,step))))\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        res = executor.map(lambda x: find_motifs(x,ms_max,diff_types),inp)\n",
    "        \n",
    "    for number_of_nodes,motifs_f1,motifs_f3 in res:\n",
    "        motifs_methods_f1[name_of_method]['Number of nodes: '+str(number_of_nodes)] = dict()\n",
    "        motifs_methods_f3[name_of_method]['Number of nodes: '+str(number_of_nodes)] = dict()\n",
    "        motifs_methods_f1[name_of_method]['Number of nodes: '+str(number_of_nodes)] = dict(list( motifs_methods_f1[name_of_method]['Number of nodes: '+str(number_of_nodes)].items()) + list(motifs_f1.items()))\n",
    "        motifs_methods_f3[name_of_method]['Number of nodes: '+str(number_of_nodes)] = dict(list(motifs_methods_f3[name_of_method]['Number of nodes: '+str(number_of_nodes)].items()) + list(motifs_f3.items()))\n",
    "    print(datetime.now()-d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-speaker",
   "metadata": {},
   "source": [
    "**конвертируем словари в матрицы частот**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка мотивов для полных графов, на случай чтоб не пересчитывать\n",
    "\n",
    "#if diff_types:\n",
    "#    with open('motifs_of_full_graphs_f1_diff.pickle','rb') as f:\n",
    "#        motifs_full_graphs_f1=pickle.load(f)\n",
    "#    with open('motifs_of_full_graphs_f3_diff.pickle','rb') as f:\n",
    "#        motifs_full_graphs_f3=pickle.load(f)\n",
    "#else:\n",
    "#    with open('motifs_of_full_graphs_f1.pickle','rb') as f:\n",
    "#        motifs_full_graphs_f1=pickle.load(f)\n",
    "#    with open('motifs_of_full_graphs_f3.pickle','rb') as f:\n",
    "#        motifs_full_graphs_f3=pickle.load(f)\n",
    "\n",
    "# Список названия мотивов. \n",
    "\n",
    "\n",
    "if diff_types:\n",
    "    names_of_all_motifs_diff=[] \n",
    "    for dataset in motifs_full_graphs_f1_diff:\n",
    "        for name_of_motif in motifs_full_graphs_f1_diff[dataset]:\n",
    "            if name_of_motif not in names_of_all_motifs_diff:\n",
    "                names_of_all_motifs_diff.append(str(name_of_motif))\n",
    "    for method in motifs_methods_f1:\n",
    "        for nn in motifs_methods_f1[method]:\n",
    "            for dataset in motifs_methods_f1[method][nn]:\n",
    "                for name_of_motif in motifs_methods_f1[method][nn][dataset]:\n",
    "                    if name_of_motif not in names_of_all_motifs_diff:\n",
    "                        names_of_all_motifs_diff.append(str(name_of_motif)) \n",
    "    names_of_all_motifs_diff=sorted(names_of_all_motifs_diff)\n",
    "else:\n",
    "    names_of_all_motifs=[] \n",
    "    for dataset in motifs_full_graphs_f1:\n",
    "        for name_of_motif in motifs_full_graphs_f1[dataset]:\n",
    "            if name_of_motif not in names_of_all_motifs:\n",
    "                names_of_all_motifs.append(str(name_of_motif))\n",
    "    for method in motifs_methods_f1:\n",
    "        for nn in motifs_methods_f1[method]:\n",
    "            for dataset in motifs_methods_f1[method][nn]:\n",
    "                for name_of_motif in motifs_methods_f1[method][nn][dataset]:\n",
    "                    if name_of_motif not in names_of_all_motifs:\n",
    "                        names_of_all_motifs.append(str(name_of_motif)) \n",
    "    names_of_all_motifs=sorted(names_of_all_motifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Составляем матрицы частот мотивов для каждого исходного графа \n",
    "if diff_types:\n",
    "    # Матрица распредления f1 частот по датасетам в исходном графе \n",
    "    X_full_f1 = np.zeros((len(graphs),len(names_of_all_motifs_diff))) #один ряд - один граф\n",
    "\n",
    "    # Матрица распредления f3 частот по датасетам в исходном графе \n",
    "    X_full_f3 = np.zeros((len(graphs),len(names_of_all_motifs_diff))) \n",
    "\n",
    "    index_list = list(range(3,ms_max+1))\n",
    "    \n",
    "    for i,(ds_name,gr) in enumerate(graphs):\n",
    "        my_dict_f1 = motifs_full_graphs_f1_diff[ds_name]\n",
    "        my_dict_f3 = motifs_full_graphs_f3_diff[ds_name]\n",
    "        sum_diсt_f1 = dict(map(lambda i: (i, sum(map(lambda e: (e[1]),filter(lambda e: str(e[0][6]) == str(i), my_dict_f1.items())))), index_list))\n",
    "        sum_diсt_f3 = dict(map(lambda i: (i, sum(map(lambda e: (e[1]),filter(lambda e: str(e[0][6]) == str(i), my_dict_f3.items())))), index_list))\n",
    "\n",
    "        X_full_f1[i] = list(map(lambda x: my_dict_f1[x] / sum_diсt_f1[int(x[6])] if x in my_dict_f1 else 0,names_of_all_motifs_diff))\n",
    "        X_full_f3[i] = list(map(lambda x: my_dict_f3[x] / sum_diсt_f3[int(x[6])] if x in my_dict_f3 else 0,names_of_all_motifs_diff))\n",
    "\n",
    "\n",
    "    with open('motifs_full_f1.npy', 'wb') as f:\n",
    "        np.save(f, X_full_f1)\n",
    "    with open('motifs_full_f3.npy', 'wb') as f:\n",
    "        np.save(f, X_full_f3)\n",
    "else:\n",
    "    X_full = np.zeros((len(graphs),len(names_of_all_motifs))) \n",
    "    index_list = list(range(3,ms_max+1))\n",
    "    for i,(ds_name,gr) in enumerate(graphs):\n",
    "        my_dict_f1 = motifs_full_graphs_f1[ds_name]\n",
    "        my_dict_f3 = motifs_full_graphs_f3[ds_name]\n",
    "        X_full[i] = list(map(lambda x: my_dict_f3[x] / my_dict_f1[x] if x in my_dict_f3 else 0,names_of_all_motifs))\n",
    "        \n",
    "    with open('motifs_full.npy', 'wb') as f:\n",
    "        np.save(f, X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Составляем матрицы частот мотивов для каждого размера сэмпла и каждого метода:\n",
    "if diff_types:\n",
    "    X_sample_f3=dict()\n",
    "    X_sample_f1=dict()\n",
    "\n",
    "    for method in methods[:1]:\n",
    "        s_max = 1 if method == RecursiveModularity else 10\n",
    "        \n",
    "        name_of_method=str(method).split('.')[-1].split(\"'\")[0]\n",
    "        \n",
    "        X_sample_f3[name_of_method] = dict()\n",
    "        X_sample_f1[name_of_method] = dict()\n",
    "        for number_of_nodes in list(range(l,r,step)):\n",
    "            X_sample_f3[name_of_method]['Number of nodes: '+str(number_of_nodes)] = np.zeros((len(graphs)*s_max,len(names_of_all_motifs_diff)))\n",
    "            X_sample_f1[name_of_method]['Number of nodes: '+str(number_of_nodes)] = np.zeros((len(graphs)*s_max,len(names_of_all_motifs_diff)))\n",
    "\n",
    "            for s in range(s_max):\n",
    "                for i,(ds_name,gr) in enumerate(graphs):\n",
    "                    my_dict_f1 = motifs_methods_f1[name_of_method]['Number of nodes: '+str(number_of_nodes)][ds_name+'_'+str(s)]\n",
    "                    my_dict_f3 = motifs_methods_f3[name_of_method]['Number of nodes: '+str(number_of_nodes)][ds_name+'_'+str(s)]\n",
    "                    sum_diсt_f1 = dict(map(lambda i: (i, sum(map(lambda e: (e[1]),filter(lambda e: str(e[0][6]) == str(i), my_dict_f1.items())))), index_list))\n",
    "                    sum_diсt_f3 = dict(map(lambda i: (i, sum(map(lambda e: (e[1]),filter(lambda e: str(e[0][6]) == str(i), my_dict_f3.items())))), index_list))\n",
    "\n",
    "                    X_sample_f3[name_of_method]['Number of nodes: '+str(number_of_nodes)][i] = list(map(lambda x: my_dict_f3[x] / sum_diсt_f3[int(x[6])] if x in my_dict_f3 else 0,names_of_all_motifs_diff))\n",
    "                    X_sample_f1[name_of_method]['Number of nodes: '+str(number_of_nodes)][i] = list(map(lambda x: my_dict_f1[x] / sum_diсt_f1[int(x[6])] if x in my_dict_f1 else 0,names_of_all_motifs_diff))\n",
    "else:\n",
    "    X_sample = dict()\n",
    "    for method in methods[:1]:\n",
    "        s_max = 1 if method == RecursiveModularity else 10\n",
    "        name_of_method=str(method).split('.')[-1].split(\"'\")[0]\n",
    "        X_sample[name_of_method] = dict()\n",
    "        for number_of_nodes in list(range(l,r,step)):\n",
    "            X_sample[name_of_method]['Number of nodes: '+str(number_of_nodes)] = np.zeros((len(graphs)*s_max,len(names_of_all_motifs)))\n",
    "            for s in range(s_max):\n",
    "                for i,(ds_name,gr) in enumerate(graphs):\n",
    "                    my_dict_f1 = motifs_methods_f1[name_of_method]['Number of nodes: '+str(number_of_nodes)][ds_name+'_'+str(s)]\n",
    "                    my_dict_f3 = motifs_methods_f3[name_of_method]['Number of nodes: '+str(number_of_nodes)][ds_name+'_'+str(s)]\n",
    "                    X_sample[name_of_method]['Number of nodes: '+str(number_of_nodes)][i] = list(map(lambda x: my_dict_f3[x] / my_dict_f1[x] if x in my_dict_f3 else 0,names_of_all_motifs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "if diff_types:\n",
    "    #тк 10 раз повторяли граф для стохастических методов, то теперь надо обрезать, чтоб сохранлся один вариант распределения для одного графа\n",
    "    X_sample_f3_to_save = dict(map(lambda e: (e[0], dict(map(lambda o: (o[0],o[1][:len(graphs)]),e[1].items()))),X_sample_f3.items()))\n",
    "    X_sample_f1_to_save = dict(map(lambda e: (e[0], dict(map(lambda o: (o[0],o[1][:len(graphs)]),e[1].items()))),X_sample_f1.items()))\n",
    "    with open('motifs_samples_f1.pickle', 'wb') as f:\n",
    "        pickle.dump(X_sample_f1_to_save, f)\n",
    "    with open('motifs_samples_f3.pickle', 'wb') as f:\n",
    "        pickle.dump(X_sample_f3_to_save,f)\n",
    "        \n",
    "else:\n",
    "    #тк 10 раз повторяли граф для стохастических методов, то теперь надо обрезать, чтоб сохранлся один вариант распределения для одного графа\n",
    "    X_sample_to_save = dict(map(lambda e: (e[0], dict(map(lambda o: (o[0],o[1][:len(graphs)]),e[1].items()))),X_sample.items()))\n",
    "    with open('motifs_samples.pickle', 'wb') as f:\n",
    "        pickle.dump(X_sample_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-birmingham",
   "metadata": {},
   "source": [
    "# MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_MSE(inp, X, X_f1,X_f3,X_samples_f1,X_samples_f3,X_samples): #возвращает MSE для мотивов f1 И f3. Без разделения на разные типы мотивов. Размеры мотивов 3 и 4\n",
    "        method,number_of_nodes = inp\n",
    "        import pickle\n",
    "        import random\n",
    "        from modules.support_functions import Utils\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        \n",
    "        MSE_f1 = []\n",
    "        MSE_f3 = []\n",
    "        MSE_nodif = []\n",
    "        \n",
    "        for i,graph in enumerate(graphs):\n",
    "            if number_of_nodes<=graph[1].number_of_nodes():\n",
    "                motifs = X_f1[i]\n",
    "                motifs_disjoint = X_f3[i]\n",
    "                motifs_nodif = X[i]\n",
    "                motifs_sample_con = X_samples_f1['Number of nodes: '+str(number_of_nodes)][i]\n",
    "                motifs_disjoint_sample_con = X_samples_f3['Number of nodes: '+str(number_of_nodes)][i]\n",
    "                motifs_nodif_sample = X_samples['Number of nodes: '+str(number_of_nodes)][i]\n",
    "                MSE_f1.append(mean_squared_error(motifs,motifs_sample_con))\n",
    "                MSE_f3.append(mean_squared_error(motifs_disjoint,motifs_disjoint_sample_con))\n",
    "                MSE_nodif.append(mean_squared_error(motifs_nodif,motifs_nodif_sample))\n",
    "            else:\n",
    "                MSE_f1.append(0)\n",
    "                MSE_f3.append(0)\n",
    "                MSE_nodif.append(0)\n",
    "        return number_of_nodes,MSE_f1, MSE_f3,MSE_nodif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "from datetime import datetime\n",
    "MSE_methods_f1=dict()\n",
    "MSE_methods_f3=dict()\n",
    "MSE_methods_nodif=dict()\n",
    "\n",
    "#with open('motifs_full_f1.npy', 'rb') as f:\n",
    " #       X_full_f1 = np.load(f)\n",
    "#with open('motifs_full_f3.npy', 'rb') as f:\n",
    " #       X_full_f3 = np.load(f)\n",
    "#with open('motifs_full.npy', 'rb') as f:\n",
    "#        X_full = np.load(f)\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    d = datetime.now()\n",
    "    name_of_method=str(method).split('.')[-1].split(\"'\")[0]\n",
    "    MSE_methods_f1.setdefault(name_of_method,dict())\n",
    "    MSE_methods_f3.setdefault(name_of_method,dict())\n",
    "    MSE_methods_nodif.setdefault(name_of_method,dict())\n",
    "    # here is a parallelization\n",
    "    inp = zip([method]*int((r - l)/step),list(range(l,r,step)))\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        res = executor.map(lambda x: find_MSE(x,X_full, X_full_f1,X_full_f3,X_sample_f1[name_of_method],X_sample_f3[name_of_method],X_sample[name_of_method]),inp)\n",
    "\n",
    "    for number_of_nodes,MSE_f1,MSE_f3,MSE_nodif in res: \n",
    "        MSE_methods_f1[name_of_method][str(number_of_nodes)]=MSE_f1\n",
    "        MSE_methods_f3[name_of_method][str(number_of_nodes)]=MSE_f3\n",
    "        MSE_methods_nodif[name_of_method][str(number_of_nodes)]=MSE_nodif\n",
    "        \n",
    "    print(datetime.now()-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(MSE_dict, name_of_method): \n",
    "    MSE = pd.DataFrame(MSE_dict , columns = list(MSE_dict.keys()))\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.suptitle(name_of_method.split(\"'\")[0], fontsize=22)\n",
    "    plt.subplot(121)\n",
    "    plt.xlabel(\"number of nodes\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    g1 = sns.boxplot(data=MSE)\n",
    "    g1.set_yscale('log')\n",
    "    plt.subplot(122)\n",
    "    plt.xlabel(\"number of nodes\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    y = list(MSE.mean())\n",
    "    x = list(map(lambda x: int(x),list(MSE.columns)))\n",
    "    g2 = sns.scatterplot(x = x, y=y)\n",
    "    g2.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in MSE_methods_f3:\n",
    "    plot(MSE_methods_f1[name],name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in MSE_methods_f1:\n",
    "    plot(MSE_methods_f1[name],name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in MSE_methods_nodif:\n",
    "    plot(MSE_methods_nodif[name],name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "mean_MSEs = []\n",
    "for name in MSE_methods_f1:\n",
    "    MSE_dict = MSE_methods_f1[name]\n",
    "    MSE = pd.DataFrame(MSE_dict , columns = list(MSE_dict.keys()))\n",
    "    y = list(MSE.mean())\n",
    "    x = list(map(lambda x: int(x),list(MSE.columns)))\n",
    "    ax = plt.scatter(x=x, y=y)\n",
    "    plt.yscale('log')\n",
    "    mean_MSEs.append(sum(y)/len(y))\n",
    "    \n",
    "plt.legend(['mean MSE of ' + str(x[0]).split('.')[-1].split(\"'\")[0] +': ' +str(np.round(x[1], decimals=3)) for x in zip(methods, mean_MSEs)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "mean_MSEs = []\n",
    "for name in MSE_methods_f3:\n",
    "    MSE_dict = MSE_methods_f3[name]\n",
    "    MSE = pd.DataFrame(MSE_dict , columns = list(MSE_dict.keys()))\n",
    "    y = list(MSE.mean())\n",
    "    x = list(map(lambda x: int(x),list(MSE.columns)))\n",
    "    ax = plt.scatter(x=x, y=y)\n",
    "    plt.yscale('log')\n",
    "    mean_MSEs.append(sum(y)/len(y))\n",
    "    \n",
    "plt.legend(['mean MSE of disjoint different types of motifs ' + str(x[0]).split('.')[-1].split(\"'\")[0] +': ' +str(np.round(x[1], decimals=3)) for x in zip(methods, mean_MSEs)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "mean_MSEs = []\n",
    "for name in MSE_methods_nodif:\n",
    "    MSE_dict = MSE_methods_nodif[name]\n",
    "    MSE = pd.DataFrame(MSE_dict , columns = list(MSE_dict.keys()))\n",
    "    y = list(MSE.mean())\n",
    "    x = list(map(lambda x: int(x),list(MSE.columns)))\n",
    "    ax = plt.scatter(x=x, y=y)\n",
    "    plt.yscale('log')\n",
    "    mean_MSEs.append(sum(y)/len(y))\n",
    "    \n",
    "plt.legend(['mean MSE of not different types of motifs' + str(x[0]).split('.')[-1].split(\"'\")[0] +': ' +str(np.round(x[1], decimals=3)) for x in zip(methods, mean_MSEs)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-compatibility",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-tiger",
   "metadata": {},
   "source": [
    "### counting number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    res = executor.map(Utils.count, list(zip(*graphs))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for n_iter in res:\n",
    "    y.append(n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "for method in methods:\n",
    "    name_of_method=str(method).split('.')[-1].split(\"'\")[0]\n",
    "    for n in list(range(l,r,step)):\n",
    "        X_f1 = X_sample_f1_to_save[name_of_method]['Number of nodes: ' + str(n)]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_f1, y, test_size=0.3)\n",
    "        \n",
    "        X_train = pd.DataFrame(X_train, columns=names_of_all_motifs_diff)\n",
    "        # Initialize CatBoostRegressor\n",
    "        model = CatBoostRegressor(iterations=100,silent=True)\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        #Get predictions\n",
    "        preds = model.predict(X_test)\n",
    "        #SHAP explainer: \n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X_train)\n",
    "        shap.plots.beeswarm(shap_values)\n",
    "        print('Different types, overlapping motifs. Method: ' ,name_of_method,' Number of nodes: ' + str(n), ' MAPE ', Utils.mean_absolute_percentage_error(y_test,preds))\n",
    "        \n",
    "        X_f3 =  X_sample_f3_to_save[name_of_method]['Number of nodes: ' + str(n)]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_f1, y, test_size=0.3)\n",
    "        X_train = pd.DataFrame(X_train, columns=names_of_all_motifs_diff)\n",
    "        #CatBoostRegressor\n",
    "        model = CatBoostRegressor(iterations=100,silent=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        #Get predictions\n",
    "        preds = model.predict(X_test)\n",
    "        #SHAP explainer:\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X_train)\n",
    "        shap.plots.beeswarm(shap_values)\n",
    "        print('Different types, disjoint motifs. Method: ' ,name_of_method,' Number of nodes: ' + str(n), ' MAPE ', Utils.mean_absolute_percentage_error(y_test,preds))\n",
    "        \n",
    "        \n",
    "        X =  X_sample_to_save[name_of_method]['Number of nodes: ' + str(n)]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        X_train = pd.DataFrame(X_train, columns=names_of_all_motifs)\n",
    "        # CatBoostRegressor\n",
    "        model = CatBoostRegressor(iterations=100,silent=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        #Get predictions\n",
    "        preds = model.predict(X_test)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X_train)\n",
    "        # summarize the effects of all the features\n",
    "        shap.plots.beeswarm(shap_values)\n",
    "        print('Not different types, disjoint motifs. Method: ' ,name_of_method,' Number of nodes: ' + str(n), ' MAPE ', Utils.mean_absolute_percentage_error(y_test,preds))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
