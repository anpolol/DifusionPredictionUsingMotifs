{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "#supernoder files:\n",
    "\n",
    "#вариант, который считает РАЗНЫЕ типы мотивов одного и того же размера\n",
    "from SuperNoder_diff_types.manager import Manager as Manager_types \n",
    "\n",
    "#вариант, который считает все мотивы одного размера вместе\n",
    "from SuperNoder.manager import Manager as Manager\n",
    "\n",
    "#!pip install ipyparallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-christopher",
   "metadata": {},
   "source": [
    "# you should \"ipcluster start -n 4\" on your local machine\n",
    "**instead of 4 you should write the number of engines on your computer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-sensitivity",
   "metadata": {},
   "source": [
    "**Read datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(r\"C:\\Users\\anpolol\\Desktop\\DifusionPredictionUsingMotifs\\Data\\reply_networks\"):\n",
    "    datasets_names = files\n",
    "    \n",
    "#in every graph there are 11 networks, all_nets consists of all nets  \n",
    "all_nets = dict()\n",
    "for dataset in datasets_names:\n",
    "    if dataset not in all_nets:\n",
    "        all_nets[dataset] = []\n",
    "        month_nets = json.load(open(r\"C:\\Users\\anpolol\\Desktop\\DifusionPredictionUsingMotifs\\Data\\reply_networks\\\\\"+str(dataset)))\n",
    "        all_nets[dataset]=all_nets[dataset] + month_nets\n",
    "        \n",
    "path=r'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-intake",
   "metadata": {},
   "source": [
    "**Запустите или клетку ниже, чтоб отфлитровать данные (дольше), или клетку ниже через одну, чтоб загрузить отфильтрованные данные (быстрее)**\n",
    "\n",
    "`type(graphs) = list of tuples: [(str(name),networkx(graph))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#фильтрация по размеру исходных графов\n",
    "def func(inp):\n",
    "        from collections import Counter\n",
    "        import networkx as nx\n",
    "        dataset,values = inp\n",
    "        name = dataset.split('.')\n",
    "        graphs=[]\n",
    "        for i,net in enumerate(values):\n",
    "\n",
    "            t=list(net.values()) #extracting nodes from json files\n",
    "            nodes = (Counter([item for sublist in t for item in sublist] + list(net.keys())))\n",
    "\n",
    "            map_nodes=dict(zip(nodes, list(range(len(nodes))))) #нумеруем ники\n",
    "\n",
    "            g = nx.Graph()\n",
    "            for node in range(len(nodes)):\n",
    "                g.add_node(node,label='Motif')\n",
    "\n",
    "            for node in net:\n",
    "                for neigh in net[node]:\n",
    "                    g.add_edge((map_nodes[node]),(map_nodes[neigh])) #first node replied to the second   \n",
    "            for j,k in enumerate(nx.connected_components(g)):\n",
    "                if (len(k)>300) and (len(k)<3000):\n",
    "                    graphs.append((name[0]+'.'+str(i)+'.'+str(j),g.subgraph(k))) #i - месяц, j - номер кмпоненты связности\n",
    "            return graphs\n",
    "        \n",
    "res = dview.map(func, list(all_nets.items())).get()\n",
    "\n",
    "graphs=[]\n",
    "for i in res:\n",
    "    graphs+=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка отфильтрованных данных\n",
    "from functools import reduce \n",
    "import pickle\n",
    "\n",
    "with open('all_graphs.pickle', 'rb') as f:\n",
    "    graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-premium",
   "metadata": {},
   "source": [
    "**Характеристика отфильтрованных графов**\n",
    "- число узлов \n",
    "- плотность\n",
    "- к-т кластеризации (общий, для графа)\n",
    "- ассортативность (по степени)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#конвертируем список кортежей в словарь для более быстрого счета\n",
    "new_names = list(map(lambda x: x.split('.')[0], list(zip(*graphs))[0]))\n",
    "new_graphs = list(zip(new_names, list(zip(*graphs))[1]))\n",
    "\n",
    "graphs_dict=dict()\n",
    "for name,g in new_graphs:\n",
    "    if name not in graphs_dict:\n",
    "        graphs_dict[name]=[]\n",
    "    graphs_dict[name].append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-thing",
   "metadata": {},
   "source": [
    "_На данном этапе можно отобрать нужное количество графов, сейчас grahs_dict содержит все компоненты связности размером от 300 до 3000 каждой из тем. Тема - ключ, список графов - значения словаря_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Характеристика графов по темам\n",
    "table_names=[]\n",
    "table_NN_mean=[]\n",
    "table_CC_mean=[]\n",
    "table_D_mean=[]\n",
    "table_DAC_mean=[]\n",
    "\n",
    "table_NN_std=[]\n",
    "table_CC_std=[]\n",
    "table_D_std=[]\n",
    "table_DAC_std=[]\n",
    "\n",
    "\n",
    "for k in graphs_dict:\n",
    "    number_of_nodes = []\n",
    "    clustering_coef=[]\n",
    "    density=[]\n",
    "    degree_assort=[]\n",
    "    graphs=[]\n",
    "\n",
    "    table_names.append(k)\n",
    "    \n",
    "    for gr in graphs_dict[k]:\n",
    "            #число узлов \n",
    "            number_of_nodes.append(gr.number_of_nodes())\n",
    "            #коэффициенты кластеризации ищутся следующим образом:\n",
    "            clustering_coef.append(np.mean(list(nx.clustering(gr).values())))\n",
    "            #плотность\n",
    "            density.append(nx.density(gr))\n",
    "            #ассортативность\n",
    "            degree_assort.append(nx.degree_assortativity_coefficient(gr))\n",
    "    table_NN_mean.append(np.mean(np.array(number_of_nodes)))\n",
    "    table_CC_mean.append(np.mean(np.array(clustering_coef)))\n",
    "    table_D_mean.append(np.mean(np.array(density)))\n",
    "    table_DAC_mean.append(np.mean(np.array(degree_assort)))\n",
    "    table_NN_std.append(np.std(np.array(number_of_nodes)))\n",
    "    table_CC_std.append(np.std(np.array(clustering_coef)))\n",
    "    table_D_std.append(np.std(np.array(density)))\n",
    "    table_DAC_std.append(np.std(np.array(degree_assort)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.DataFrame({'Name of dataset' : table_names,\n",
    "                                'Number of nodes (NN)' :table_NN_mean,\n",
    "                                'Clustering coefficient (CC)': table_CC_mean,'Density(D)':table_D_mean, 'Degree assortativity coefficient(DAC)':table_DAC_mean})\n",
    "std_df = pd.DataFrame({'Name of dataset' : table_names,\n",
    "                                'Number of nodes (NN)' :table_NN_std,\n",
    "                                'Clustering coefficient (CC)': table_CC_std,'Density(D)':table_D_std, 'Degree assortativity coefficient(DAC)':table_DAC_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-station",
   "metadata": {},
   "source": [
    "# **Motif counter - SuperNoder** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-committee",
   "metadata": {},
   "source": [
    "**arguments for SuperNoder:**\n",
    "\n",
    "-n,  --nodes-file <filename> MANDATORY The list of nodes. Node id and label for each row separated by a space\\\n",
    "-e,  --edges-file <filename> MANDATORY The list of edges. One edge for each row.\\\n",
    "-m,  --method <method> OPTIONAL The heuristic to use in order to maximize motifs. DEFAULT: h1 \\\n",
    "-tn, --type-of-network <type> OPTIONAL \tThe type of network. It can be chosen from [direct, undirect]. DEFAULT: undirect\\ \n",
    "-th, --threshold <threshold> OPTIONAL The threshold to hold over-represented motifs.\\\n",
    "-ms, --motif-size <size> OPTIONAL The size of motifs. It must be greater or equal to 3. DEFAULT: 3\\\n",
    "-h1tr, --h1-times-repetition <times> OPTIONAL \\tThe number of repetition of h1. DEFAULT: 1\\\n",
    "-ss, --samples-size <sample_size> OPTIONAL The size of samples for heuristics h4 and h5. DEFAULT: 100\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_disjoint_motifs(g):\n",
    "    from SuperNoder.manager import Manager as Manager_types \n",
    "    arg_tn='undirect'\n",
    "    arg_th = '1' \n",
    "    arg_m = 'h1'\n",
    "    arg_ss = '100'\n",
    "    arg_h1tr = 1 \n",
    "    distributions = {}\n",
    "    distributions_disjoint = {}\n",
    "    for arg_ms in range(3,5):\n",
    "        argv=[' ','-g',g[1],'-th',arg_th,'-ms',arg_ms,'-m',arg_m,\\\n",
    "      '-h1tr',arg_h1tr,'-ss',arg_ss]\n",
    "        m = Manager_types(argv)\n",
    "        distribution_f1 = m.run()\n",
    "        distribution_disjoint = m.disjoint_finder()\n",
    "        distributions = dict(list(distributions.items()) + list(distribution_f1.items()))\n",
    "        distributions_disjoint = dict(list(distributions_disjoint.items()) + list(distribution_disjoint.items()))\n",
    "    return  g[0],distributions_disjoint,distributions#первое значение словаря - тип мотива или размер мотива, второе значение - количество таких мотивов в графе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a parallelization\n",
    "res = dview.map(find_disjoint_motifs,graphs[:2]).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributions of motifs for every net:\n",
    "Motif_f1 = dict()\n",
    "Motif_f3 = dict()\n",
    "for (name,f3,f1) in res:\n",
    "    Motif_f3[name] = f3\n",
    "    Motif_f1[name] = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-assistant",
   "metadata": {},
   "source": [
    "**составляем матрицу входных данных таким образом, чтоб она была одной и той же длины для любого датасета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_of_all_motifs=[] #list of all motif types in all datasets\n",
    "for dataset in Motif_f3:\n",
    "    for name_of_motif in Motif_f3[dataset]:\n",
    "        if name_of_motif not in names_of_all_motifs:\n",
    "            names_of_all_motifs.append(str(name_of_motif))\n",
    "names_of_all_motifs=sorted(names_of_all_motifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(args_e),len(names_of_all_motifs)))\n",
    "for i,d in enumerate(args_e):\n",
    "    for k,m in enumerate(names_of_all_motifs):\n",
    "        if m in Motif_f3[d]:\n",
    "            X[i][k] = Motif_f3[d][m]/sum(Motif_f3[d].values())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-portugal",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from littleballoffur import BreadthFirstSearchSampler, \\\n",
    "                            PageRankBasedSampler, \\\n",
    "                            CommunityStructureExpansionSampler, \\\n",
    "                            MetropolisHastingsRandomWalkSampler, \\\n",
    "                            RandomWalkSampler, \\\n",
    "                            RandomNodeNeighborSampler, \\\n",
    "                            CommonNeighborAwareRandomWalkSampler\n",
    "methods = [\n",
    "           #random node sampling\n",
    "           DegreeBasedSampler,\n",
    "           PageRankBasedSampler,\n",
    "\n",
    "           #Random Edge Sampling\n",
    "           RandomEdgeSampler,\n",
    "    \n",
    "           SnowBallSampler,\n",
    "           ForestFireSampler,\n",
    "           CommunityStructureExpansionSampler,\n",
    "           ShortestPathSampler,\n",
    "            #Random-Walks Dased\n",
    "           RandomWalkSampler,\n",
    "           RandomWalkWithJumpSampler,\n",
    "           MetropolisHastingsRandomWalkSampler,\n",
    "           NonBackTrackingRandomWalkSampler,\n",
    "           CirculatedNeighborsRandomWalkSampler,\n",
    "           CommonNeighborAwareRandomWalkSampler,\n",
    "           LoopErasedRandomWalkSampler\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "arg_tn='undirect'\n",
    "arg_th = '1' \n",
    "arg_m = 'h1'\n",
    "arg_ss = '100'\n",
    "arg_h1tr = 1 \n",
    "\n",
    "graph_samples = []\n",
    "MSE_samples = []\n",
    "graph = graphs[0]\n",
    "distrubutions_full=dict()\n",
    "for arg_ms in range(3,5):\n",
    "    argv=[' ','-g',graph[1],'-th',arg_th,'-ms',arg_ms,'-m',arg_m,'-h1tr',arg_h1tr,'-ss',arg_ss]      \n",
    "    m = Manager_types(argv)\n",
    "    motif_full_f1 = m.run()\n",
    "    distrubutions_full = dict(list(distrubutions_full.items()) + list(motif_full_f1.items()))\n",
    "distrubutions_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motifs_to_vec(motifs1, motifs2):\n",
    "    names_of_all_motifs=[] #list of all motif types in all datasets\n",
    "    for name_of_motif in motifs1:\n",
    "        if name_of_motif not in names_of_all_motifs:\n",
    "            names_of_all_motifs.append(str(name_of_motif))\n",
    "    for name_of_motif in motifs2:\n",
    "        if name_of_motif not in names_of_all_motifs:\n",
    "            names_of_all_motifs.append(str(name_of_motif))\n",
    "    names_of_all_motifs=sorted(names_of_all_motifs)\n",
    "    \n",
    "    new_motifs = np.zeros(2,len(names_of_all_motifs))\n",
    "    for k,m in enumerate(names_of_all_motifs):\n",
    "        if m in motifs1:\n",
    "            new_motifs[0][k] = motifs1[m]\n",
    "            new_motifs[1][k] = motifs2[m]\n",
    "                \n",
    "    return new_motifs[0], new_motifs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nodes = int(0.5*graph[1].number_of_nodes()) \n",
    "for method in methods:\n",
    "    sampler = method(number_of_nodes)\n",
    "    sample = sampler.sample(graph[1])\n",
    "    distribution_sample =dict()\n",
    "    for arg_ms in range(3,5):\n",
    "        argv=[' ','-g',graph[1],'-th',arg_th,'-ms',arg_ms,'-m',arg_m,'-h1tr',arg_h1tr,'-ss',arg_ss]      \n",
    "        m = Manager_types(argv)\n",
    "        motif_sample_f1 = m.run()\n",
    "        distribution_sample = dict(list(distributions_sample.items()) + list(motif_sample_f1.items()))\n",
    "    MSE.append(mean_squared_error(motifs_full,motifs_sample))\n",
    "print('MSE: ', MSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
