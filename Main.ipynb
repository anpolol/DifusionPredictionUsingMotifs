{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affecting-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import networkx as nx\n",
    "import datetime\n",
    "\n",
    "#supernoder files:\n",
    "\n",
    "#вариант, который считает РАЗНЫЕ типы мотивов одного и того же размера\n",
    "from SuperNoder_diff_types.manager import Manager as Manager_types \n",
    "\n",
    "\n",
    "#вариант, который считает все мотивы одного размера вместе\n",
    "from SuperNoder.manager import Manager as Manager\n",
    "\n",
    "#!pip install ipyparallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-huntington",
   "metadata": {},
   "source": [
    "**First of all will convert every dataset to a txt file with nodes as numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "binary-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's choose graphs randomly. k - number of graphs\n",
    "for root, dirs, files in os.walk(r\"C:\\Users\\anpolol\\Desktop\\DifusionPredictionUsingMotifs\\Data\\reply_networks\"):\n",
    "    datasets_names = random.choices(files,k=12)\n",
    "    \n",
    "#in every graph there are 11 networks, all_nets consists of all nets  \n",
    "all_nets = dict()\n",
    "for dataset in datasets_names:\n",
    "    if dataset not in all_nets:\n",
    "        all_nets[dataset] = []\n",
    "        month_nets = json.load(open(r\"C:\\Users\\anpolol\\Desktop\\DifusionPredictionUsingMotifs\\Data\\reply_networks\\\\\"+str(dataset)))\n",
    "        all_nets[dataset]=all_nets[dataset] + month_nets\n",
    "        \n",
    "#Create txt files from json\n",
    "path=r'Data/'\n",
    "for dataset in all_nets:\n",
    "    name = dataset.split('.')\n",
    "    for i,net in enumerate(all_nets[dataset]):\n",
    "        \n",
    "        #Create nodes file\n",
    "        t=list(net.values())\n",
    "        nodes = (Counter([item for sublist in t for item in sublist] + list(net.keys()))) \n",
    "        map_nodes=dict(zip( nodes , list(range(len(nodes))))) #нумеруем ники\n",
    "        if not os.path.exists(path+name[0]+\"_\"+str(i)+\"_nodes\"+\".txt\"):\n",
    "            file_nodes = open(path+name[0]+\"_\"+str(i)+\"_nodes\"+\".txt\", \"w+\")\n",
    "            for j in range(len(nodes)):\n",
    "                file_nodes.write(str(j)+' A'+'\\n')\n",
    "            file_nodes.close()\n",
    "        \n",
    "        #Create edges files\n",
    "        if not os.path.exists(path+name[0]+\"_\"+str(i)+\".txt\"):\n",
    "            file= open(path+name[0]+\"_\"+str(i)+\".txt\", \"w+\") \n",
    "            for node in net:\n",
    "                for neigh in net[node]:\n",
    "                    file.write(str(map_nodes[node])+' '+str(map_nodes[neigh])+'\\n') #first node replied to the second    \n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-letter",
   "metadata": {},
   "source": [
    "**Some statistics for graphs**:\n",
    "- число узлов \n",
    "- плотность\n",
    "- к-т кластеризации (общий, для графа)\n",
    "- ассортативность (по степени)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-clone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes (NN), Clustering coefficient (CC), Density(D), Degree assortativity coefficient(DAC) \n",
      "\n",
      "DATASET:  Drifting.json \n",
      "\n",
      "mean:  NN: 280.0909, CC: 0.0294, D: 0.0054, DAC: -0.1136 \n",
      "\n",
      "std:  NN: 31.5694, CC: 0.0085, D: 0.0005, DAC: 0.0515 \n",
      "\n",
      "DATASET:  TrueAtheism.json \n",
      "\n",
      "mean:  NN: 1727.6364, CC: 0.0313, D: 0.0013, DAC: -0.0539 \n",
      "\n",
      "std:  NN: 260.7305, CC: 0.0064, D: 0.0002, DAC: 0.0283 \n",
      "\n",
      "DATASET:  gopro.json \n",
      "\n",
      "mean:  NN: 1125.7273, CC: 0.0286, D: 0.0015, DAC: -0.0581 \n",
      "\n",
      "std:  NN: 123.8673, CC: 0.0070, D: 0.0001, DAC: 0.0266 \n",
      "\n",
      "DATASET:  beer.json \n",
      "\n",
      "mean:  NN: 3383.5455, CC: 0.0234, D: 0.0006, DAC: 0.0097 \n",
      "\n",
      "std:  NN: 256.6045, CC: 0.0032, D: 0.0000, DAC: 0.0197 \n",
      "\n",
      "DATASET:  reptiles.json \n",
      "\n",
      "mean:  NN: 412.3636, CC: 0.0405, D: 0.0048, DAC: -0.0527 \n",
      "\n",
      "std:  NN: 13.5329, CC: 0.0125, D: 0.0004, DAC: 0.0352 \n",
      "\n",
      "DATASET:  letsplay.json \n",
      "\n",
      "mean:  NN: 957.7273, CC: 0.0722, D: 0.0049, DAC: -0.0804 \n",
      "\n",
      "std:  NN: 63.0528, CC: 0.0169, D: 0.0002, DAC: 0.0253 \n",
      "\n",
      "DATASET:  SoccerBetting.json \n",
      "\n",
      "mean:  NN: 269.3636, CC: 0.1096, D: 0.0134, DAC: -0.1221 \n",
      "\n",
      "std:  NN: 77.4670, CC: 0.0443, D: 0.0050, DAC: 0.0496 \n",
      "\n",
      "DATASET:  gamingpc.json \n",
      "\n",
      "mean:  NN: 801.6364, CC: 0.0359, D: 0.0022, DAC: -0.0883 \n",
      "\n",
      "std:  NN: 90.0286, CC: 0.0071, D: 0.0002, DAC: 0.0247 \n",
      "\n",
      "DATASET:  esports.json \n",
      "\n",
      "mean:  NN: 195.0000, CC: 0.0399, D: 0.0084, DAC: -0.0869 \n",
      "\n",
      "std:  NN: 41.4773, CC: 0.0137, D: 0.0018, DAC: 0.0412 \n",
      "\n",
      "DATASET:  zelda.json \n",
      "\n",
      "mean:  NN: 2004.4545, CC: 0.0227, D: 0.0008, DAC: -0.0265 \n",
      "\n",
      "std:  NN: 341.6323, CC: 0.0064, D: 0.0001, DAC: 0.0417 \n",
      "\n",
      "DATASET:  taiwan.json \n",
      "\n",
      "mean:  NN: 294.2727, CC: 0.0556, D: 0.0078, DAC: -0.0266 \n",
      "\n",
      "std:  NN: 35.1855, CC: 0.0093, D: 0.0010, DAC: 0.0675 \n",
      "\n",
      "DATASET:  weather.json \n",
      "\n",
      "mean:  NN: 343.4545, CC: 0.0192, D: 0.0039, DAC: -0.0076 \n",
      "\n",
      "std:  NN: 112.0092, CC: 0.0074, D: 0.0008, DAC: 0.0531 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#построение графа из файла с ребрами\n",
    "print('Number of nodes (NN), Clustering coefficient (CC), Density(D), Degree assortativity coefficient(DAC) \\n')\n",
    "for dataset in all_nets:\n",
    "    print('DATASET: ', dataset,'\\n')\n",
    "    name = dataset.split('.')\n",
    "    number_of_nodes = []\n",
    "    clustering_coef=[]\n",
    "    density=[]\n",
    "    degree_assort=[]\n",
    "    for i,net in enumerate(all_nets[dataset]):\n",
    "        g = nx.read_edgelist(path+name[0]+\"_\"+str(i)+\".txt\",delimiter=' ', create_using=nx.DiGraph(), nodetype = int)\n",
    "        #число узлов \n",
    "        number_of_nodes.append(g.number_of_nodes())\n",
    "        #коэффициенты кластеризации ищутся следующим образом:\n",
    "        clustering_coef.append(np.mean(list(nx.clustering(g).values())))\n",
    "        #плотность\n",
    "        density.append(nx.density(g))\n",
    "        #ассортативность\n",
    "        degree_assort.append(nx.degree_assortativity_coefficient(g))\n",
    "    log = 'NN: {:.4f}, CC: {:.4f}, D: {:.4f}, DAC: {:.4f}'\n",
    "    print('mean: ', log.format(np.mean(np.array(number_of_nodes)) ,np.mean(np.array(clustering_coef)) ,np.mean(np.array(density)),np.mean(np.array(degree_assort))),'\\n')\n",
    "    print('std: ',log.format(np.std(np.array(number_of_nodes)) ,np.std(np.array(clustering_coef)) ,np.std(np.array(density)),np.std(np.array(degree_assort))),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-station",
   "metadata": {},
   "source": [
    "# **Motif counter - SuperNoder** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-committee",
   "metadata": {},
   "source": [
    "**arguments for SuperNoder:**\n",
    "\n",
    "-n,  --nodes-file <filename> MANDATORY The list of nodes. Node id and label for each row separated by a space\\\n",
    "-e,  --edges-file <filename> MANDATORY The list of edges. One edge for each row.\\\n",
    "-m,  --method <method> OPTIONAL The heuristic to use in order to maximize motifs. DEFAULT: h1 \\\n",
    "-tn, --type-of-network <type> OPTIONAL \tThe type of network. It can be chosen from [direct, undirect]. DEFAULT: undirect\\ \n",
    "-th, --threshold <threshold> OPTIONAL The threshold to hold over-represented motifs.\\\n",
    "-ms, --motif-size <size> OPTIONAL The size of motifs. It must be greater or equal to 3. DEFAULT: 3\\\n",
    "-h1tr, --h1-times-repetition <times> OPTIONAL \\tThe number of repetition of h1. DEFAULT: 1\\\n",
    "-ss, --samples-size <sample_size> OPTIONAL The size of samples for heuristics h4 and h5. DEFAULT: 100\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-amazon",
   "metadata": {},
   "source": [
    "# you should \"ipcluster start -n 4\" on your local machine\n",
    "**instead of 4 you should write the number of engines on your computer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clear-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informational-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_e=[]\n",
    "for dataset in all_nets:\n",
    "    name = dataset.split('.')\n",
    "    for i,net in enumerate(all_nets[dataset]):\n",
    "        args_e.append(path+name[0]+\"_\"+str(i)+\".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accepted-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(arg_e):\n",
    "    from SuperNoder_diff_types.manager import Manager as Manager_types \n",
    "    arg_tn='direct'\n",
    "    arg_th = '1' \n",
    "    arg_m = 'h1'\n",
    "    arg_ss = '100'\n",
    "    arg_h1tr = 1 \n",
    "    name=arg_e.split('.')\n",
    "    arg_n = name[0]+\"_nodes\"+\".txt\"\n",
    "    distributions = {}\n",
    "    distributions_disjoint = {}\n",
    "    \n",
    "    for arg_ms in range(3,9):\n",
    "        argv=[' ','-e',arg_e, '-n',arg_n, '-tn',arg_tn,'-th',arg_th,'-ms',arg_ms,'-m',arg_m,\\\n",
    "      '-h1tr',arg_h1tr,'-ss',arg_ss]\n",
    "        m = Manager_types(argv)\n",
    "        distribution_disjoint, distribution_f1 = m.run()\n",
    "        distributions = dict(list(distributions.items()) + list(distribution_f1.items()))\n",
    "        distributions_disjoint = dict(list(distributions_disjoint.items()) + list(distribution_disjoint.items()))\n",
    "    return  arg_e,distributions_disjoint,distributions#первое значение словаря - тип мотива или размер мотива, второе значение - количество таких мотивов в графе\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fiscal-generic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c параллелизацией ушло 0:00:05.417999\n"
     ]
    }
   ],
   "source": [
    "t = datetime.datetime.now()\n",
    "res = dview.map(func,args_e[:2]).get()\n",
    "print('c параллелизацией на 4 ядра было затрачено времени: ',(datetime.datetime.now()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "enormous-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "#построение распределения мотивов для каждой сети:\n",
    "Motif_f1 = dict()\n",
    "Motif_f3 = dict()\n",
    "for (name,f3,f1) in res:\n",
    "    Motif_f3[name] = f3\n",
    "    Motif_f1[name] = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-assistant",
   "metadata": {},
   "source": [
    "**составляем матрицу входных данных таким образом, чтоб она была одной и той же длины для любого датасета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "third-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_of_all_motifs=[] #list of all motif types in all datasets\n",
    "for dataset in Motif_f3:\n",
    "    for name_of_motif in Motif_f3[dataset]:\n",
    "        if name_of_motif not in names_of_all_motifs:\n",
    "            names_of_all_motifs.append(str(name_of_motif))\n",
    "names_of_all_motifs=sorted(names_of_all_motifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "noted-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(args_e),len(names_of_all_motifs)))\n",
    "for i,d in enumerate(args_e):\n",
    "    for k,m in enumerate(names_of_all_motifs):\n",
    "        if m in Motif_f3[d]:\n",
    "            X[i][k] = Motif_f3[d][m]/sum(Motif_f3[d].values())  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
